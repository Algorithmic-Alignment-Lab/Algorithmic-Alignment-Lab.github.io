---
layout: default
permalink: /research/
---

{% include nav.html %}

## Research

Find us on [Github](https://github.com/Algorithmic-Alignment-Lab).

### 2022

RÃ¤ukur, T., Ho, A., Casper, S., & Hadfield-Menell, D. (2022). [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://arxiv.org/abs/2207.13243). arXiv preprint arXiv:2207.13243. [BibTeX](https://scholar.googleusercontent.com/scholar.bib?q=info:6IDnKqjNOrcJ:scholar.google.com/&output=citation&scisdr=CgUBYGTzEPyMg5PIZuc:AAGBfm0AAAAAYxjOfudLK6ychKhzX_GGjk7JydhRaQBs&scisig=AAGBfm0AAAAAYxjOfpUSteParaaZUb0Baq11kd8bT7oX&scisf=4&ct=citation&cd=-1&hl=en)

Casper, S., Hadfield-Menell, D., Kreiman, G (2022). [White-Box Adversarial Policies in Deep Reinforcement Learning](https://arxiv.org/abs/2209.02167). 

Christoffersen, P.J.K., Haupt, A.A, Hadfield-Menell, D. (2022). [Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL](https://arxiv.org/abs/2208.10469). [BibTeX](https://scholar.googleusercontent.com/scholar.bib?q=info:rctroivbpiAJ:scholar.google.com/&output=citation&scisdr=CgUBYGTzEPyMg5PIHNw:AAGBfm0AAAAAYxjOBNz2-vpDhjCI_wJ1FUgMgTwUEa8f&scisig=AAGBfm0AAAAAYxjOBPuUsEPypykSIeu3v7C_ZNMSKwx8&scisf=4&ct=citation&cd=-1&hl=en)

Yew, R.J. and Hadfield-Menell, D. (2022). [A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems](https://dl.acm.org/doi/10.1145/3514094.3534130). In Proceedings of the 5th AAAI/ACM Conference on AI, Ethics, and Society. [BibTeX](https://scholar.googleusercontent.com/scholar.bib?q=info:Zy8cJGbw9QUJ:scholar.google.com/&output=citation&scisdr=CgWTYX5AEPyMg45o47g:AAGBfm0AAAAAYwVu-7hfL7sgjbex8wF3U-g2nDKsY20o&scisig=AAGBfm0AAAAAYwVu-y80HvtCEX2eXNg2NM7Ki7kE-BiC&scisf=4&ct=citation&cd=-1&hl=en)

Casper, S., Nadeau, M., Hadfield-Menell, D, & Kreiman, G (2022). [Robust Feature-Level Adversaries are Interpretability Tools](https://arxiv.org/abs/2110.03605). [BibTeX](https://dblp.uni-trier.de/rec/journals/corr/abs-2110-03605.html?view=bibtex)   


