<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=">
    <link rel="shortcut icon" type="image/x-icon" href="favicon_io/favicon.ico">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Algorithmic Alignment Group | Researching frameworks for human-aligned AI @ MIT CSAIL.</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Algorithmic Alignment Group" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Researching frameworks for human-aligned AI @ MIT CSAIL." />
<meta property="og:description" content="Researching frameworks for human-aligned AI @ MIT CSAIL." />
<link rel="canonical" href="https://thestephencasper.github.io/research/" />
<meta property="og:url" content="https://thestephencasper.github.io/research/" />
<meta property="og:site_name" content="Algorithmic Alignment Group" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Algorithmic Alignment Group" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Researching frameworks for human-aligned AI @ MIT CSAIL.","headline":"Algorithmic Alignment Group","url":"https://thestephencasper.github.io/research/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          

          <a href="/"><img style="float: left; padding-right: 20px; border: 0px;" src="/docs/assets/logo.png" width="68" height="68"></a><h1 id="project_title">Algorithmic Alignment Group</h1>
          <h3 id="project_tagline">Researching frameworks for human-aligned AI @ MIT CSAIL.</h3>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
<!--<h1>Group Name and Logo</h1>-->

<!--<head>-->
<!--    <meta charset="utf-8">-->
<!--    <link rel="stylesheet" href="_includes/styles.css">-->
<!--</head>-->

<center>
    <nav>
        <ul class="nav__links">
            <li><a href="/"><h3>Home</h3></a></li>
            <li><a href="/team/"><h3>Team</h3></a></li>
            <li><a href="/research/"><h3>Research</h3></a></li>
            <li><a href="/contact/"><h3>Contact</h3></a></li>
        </ul>
    </nav>
</center>

<h2 id="research">Research</h2>

<p>Find us on <a href="https://github.com/Algorithmic-Alignment-Lab">Github</a>.</p>

<h3 id="2024">2024</h3>

<h4 id="papers">Papers</h4>

<p>Sheshadri, A., Ewart, A., Guo, P., Lynch, A., Wu, C., Hebbar, V., Sleight, H., Cooper Stickland A., Perez, E., Hadfield-Menell, D., &amp; Casper, S. (2024). <a href="">Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs.</a> arXiv preprint arXiv:2407.15549.<a href="https://arxiv.org/abs/2407.15549">BibTeX</a></p>

<p>Casper, S., Yun, J., Baek, J., Jung, Y., Kim, M., Kwon, K., … &amp; Hadfield-Menell, D. (2024). <a href="https://arxiv.org/abs/2404.02949">The SaTML’24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability.</a> arXiv preprint arXiv:2404.02949. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:x4JPMDBv3TgJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGidL9yEIaMg7yzD8o:AFWwaeYAAAAAZhK1F8rTj_IS5eT03h-duUYWftU&amp;scisig=AFWwaeYAAAAAZhK1F3X_AdsFRCzxa2qeqTA7oco&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a></p>

<p>Casper, S., Schulze, L., Patel, O., Hadfield-Menell, D. (2024) <a href="https://arxiv.org/abs/2403.05030">Defending Against Unforeseen Failure Modes with Latent Adversarial Training</a> arXiv preprint: ariXiv:2403.05030 <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:epBKB-Umi1MJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGidL9yEIaMg7yzGbk:AFWwaeYAAAAAZhK1AbkrDvAbxi7MLbnZ3d2H_IQ&amp;scisig=AFWwaeYAAAAAZhK1Adxqs0S_OPGxJ_dqfe0Dy8g&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a></p>

<p>Lynch, A., Guo, P., Ewart, A.*, Casper, S., Hadfield-Menell, D. (2024). <a href="https://arxiv.org/abs/2402.16835">Eight Methods to Evaluate Robust Unlearning in LLMs.</a> arXiv preprint: ariXiv:2402.16835. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Ea0aPUr5uVkJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGidHcwEIaMgEBgWCQ:AFWwaeYAAAAAZe5mQCQ6iaKYzjvpwp75W_fMaUw&amp;scisig=AFWwaeYAAAAAZe5mQE3IhnbAnh6AjPHXsWXKUlU&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a></p>

<p>Casper, S., Ezell, C., Siegmann, C., Kolt, N., Curtis, T., Bucknall, B., Haupt, A., Wei, K., Scheurer, J., Hobbhahn, M., Sharkey, L., Krishna, S., Von Hagen, M., Alberti, S., Chan, A., Sun, Q., Gerovitch, M., Bau, D., Tegmark, M., Krueger, D., Hadfield-Menell, D. (2024) <a href="https://arxiv.org/abs/2401.14446">Black-Box Access is Insufficient for Rigorous AI Audits.</a> Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2024. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:SIgJw-M0K-oJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGidHcwEIaMgEBgcRc:AFWwaeYAAAAAZe5maRd4xJkKwUDBF4WW2w-7eGo&amp;scisig=AFWwaeYAAAAAZe5maX_sXB_drOinR6kTuVtJK5E&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a></p>

<h3 id="2023">2023</h3>

<h4 id="papers-1">Papers</h4>

<p>Liu, K., Casper, S., Hadfield-Menell, D., Andreas., J. (2023) <a href="https://arxiv.org/abs/2312.03729">Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?</a> EMNLP, 2023. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5BycjoRATvEJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEt6tmSELrotScCUas:AFWwaeYAAAAAZbgESatLgBg16vDIEXl3VefLxy4&amp;scisig=AFWwaeYAAAAAZbgESbnvkMNVJSF7hh-hVtUAyTs&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BixTex</a></p>

<p>Casper S., Davies, X., Shi, C., Gilbert, T., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T., Marks, S., Segerie, C., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Bıyık, E., Dragan, A., Krueger, D., Sadigh, D., &amp; Hadfield-Menell, D. (2023) <a href="https://arxiv.org/abs/2307.15217">Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</a>. arXiv preprint: arXiv:2307.15217. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:RH-S-PbNWxwJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK8gUM:AFWwaeYAAAAAZQ26mUOuSmaMoaBbXw5x8Xcd9_c&amp;scisig=AFWwaeYAAAAAZQ26mfH5WdcdIFu_Sqw5gMb1WgY&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Yew, R. J. &amp; Hadfield-Menell, D. (2023). <a href="https://genlaw.github.io/CameraReady/30.pdf?fbclid=IwAR3kZNuMD4ktqDyIDH_0IdYFn7hDdGVhJ31vIHLr2gi_ZQ8JuRhVtoba7dI">Break It Till You Make It: Limitations of Copyright Liability Under A Pre-training Paradigm of AI Development</a>. Featured at the first annual Generative AI + Law Workshop at ICML 2023. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:aYUif0qWxs0J:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK8ZzY:AFWwaeYAAAAAZQ26fzYksC9xYIesKNZZFvOJohw&amp;scisig=AFWwaeYAAAAAZQ26f9ap7pAVaIo6_NUe7GsG9b4&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>. ***<a href="https://genlaw.github.io/papers.html#break-it-till-you-make-it-limitations-of-copyright-liability-under-a-pre-training-paradigm-of-ai-development">Spotlight paper award</a>***</p>

<p>Casper, S., Guo, Z., Mogulothu, S., Marinov, Z., Deshpande, C., Yew, R. J., Dai, Z., &amp; Hadfield-Menell, D. (2023). <a href="https://arxiv.org/abs/2307.04028">Measuring the Success of Diffusion Models at Imitating Human Artists</a>. Featured at the first annual Generative AI + Law Workshop at ICML 2023. <a href="Measuring the Success of Diffusion Models at Imitating Human Artists">BibTeX</a>. ***<a href="https://genlaw.github.io/papers.html#measuring-the-success-of-diffusion-models-at-imitating-human-artists">Spotlight paper award</a>***</p>

<p>Casper, S., Lin, J., Kwon, J., Culp, G., &amp; Hadfield-Menell, D. (2023). <a href="https://arxiv.org/abs/2306.09442">Explore, Establish, Exploit: Red Teaming Language Models from Scratch.</a> arXiv preprint arXiv:2306.09442. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:rzNKauKCM_QJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK8QU0:AFWwaeYAAAAAZQ26WU2CMr9A1UaPA0PZoq0p4pA&amp;scisig=AFWwaeYAAAAAZQ26WWSdwIqH9zq2bj1Pr96KjbE&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Zhang, B. H., Farina, G., Anagnostides, I., Cacciamani, F., McAleer, S. M., Haupt, A. A., … &amp; Sandholm, T. (2023). <a href="https://arxiv.org/abs/2306.05221">Steering No-Regret Learners to Optimal Equilibria.</a> arXiv preprint arXiv:2306.05221. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:em_edOqxDFUJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK8Jng:AFWwaeYAAAAAZQ26PngHalv3kHKFmUIpCSth1xA&amp;scisig=AFWwaeYAAAAAZQ26Pur57wTTkyB-WAMp1Ek9LlM&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Zhang, B. H., Farina, G., Anagnostides, I., Cacciamani, F., McAleer, S. M., Haupt, A. A., … &amp; Sandholm, T. (2023). <a href="https://arxiv.org/abs/2306.05216">Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games.</a> Advances in Neural Information Processing Systems, 36. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:At-MfwxtpCcJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK8FW4:AFWwaeYAAAAAZQ26DW5X9Vo8YMBQzuulukz-Svg&amp;scisig=AFWwaeYAAAAAZQ26DWMq5gPAi9GjQnH-74KWMg4&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Yew, R.J., Curtis, T.L., Leake, M., Podimata, C., Hadfield-Menell, D. (2023). <a href="https://cornell.app.box.com/s/gtqnbjiial0kzcqcr5awqdfz10hv0c92">Policy Paths Toward an Understanding of AI Interfaces: A Case Study on Recommendation Platforms.</a> 2023 ACM CHI Designing Technology and Policy Simultaneously Workshop.</p>

<p>Casper, S., Li, Y., Li, J., Bu, T., Zhang, K., Hariharan, K., Hadfield-Menell, D., (2023). <a href="https://arxiv.org/abs/2302.10894">Red Teaming Deep Neural Networks with Feature Synthesis Tools.</a> Advances in Neural Information Processing Systems, 36. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:BNZOzOIqT1IJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK_rXI:AFWwaeYAAAAAZQ25tXK_FhpczacuVjgj86lAWHc&amp;scisig=AFWwaeYAAAAAZQ25tapRK7e3J8ce9czQQNbTS18&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Haupt, A., Hadfield-Menell, D., &amp; Podimata, C. (2023). <a href="https://arxiv.org/abs/2302.06559">Recommending to Strategic Users.</a> arXiv preprint arXiv:2302.06559. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:TvVSadb4eCUJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK_Dhw:AFWwaeYAAAAAZQ25Fhx27stTN2r6MyhL6Wcwv94&amp;scisig=AFWwaeYAAAAAZQ25Fg4cCekUAhITAKQ5YgtaTio&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">Bibtex</a>.</p>

<h4 id="resources">Resources</h4>

<p><a href="https://github.com/Algorithmic-Alignment-Lab/contracts">Contracts Library</a> for contract-based multiagent RL. Christoffersen, P.J.K., Haupt, A.A, Damani, M.</p>

<p><a href="https://github.com/Algorithmic-Alignment-Lab/CommonClaim">CommonClaim Dataset</a> of 20k statements labeled by humans as common-knowledge true, common-knowledge false, and neither. Casper, S., Lin, J., Kwon, J., Culp, G., &amp; Hadfield-Menell, D.</p>

<h3 id="2022">2022</h3>

<h4 id="papers-2">Papers</h4>

<p>Casper, S., Hariharan, K., Hadfield-Menell, D., (2022). <a href="https://arxiv.org/abs/2211.10024">Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks.</a> arXiv preprint. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:PK3dFIQRxXsJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK-9wo:AFWwaeYAAAAAZQ247wo4GXoDnqP-_S51IozH5dE&amp;scisig=AFWwaeYAAAAAZQ2471Un51ho87aYF2lSozOtdXU&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>. *** <em><a href="https://neurips2022.mlsafety.org/">Best paper award</a> — 2022 NeurIPS Machine Learning Safety Workshop</em> ***</p>

<p>Curmei, M., Haupt, A. A., Recht, B., &amp; Hadfield-Menell, D. (2022, September). <a href="https://dl.acm.org/doi/abs/10.1145/3523227.3546778">Towards Psychologically-Grounded Dynamic Preference Models</a>. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 35-48). <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:WOxa2WDGuTcJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK-1vU:AFWwaeYAAAAAZQ24zvX7k5p1QSO-ZuVVpTqrMtk&amp;scisig=AFWwaeYAAAAAZQ24zgTj6mA_XfKnu2CH7MTIJjQ&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Räuker, T., Ho, A., Casper, S., &amp; Hadfield-Menell, D. (2022). <a href="https://arxiv.org/abs/2207.13243">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks</a>. SATML 2023. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:6IDnKqjNOrcJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgUBYGTzEPyMg5PIZuc:AAGBfm0AAAAAYxjOfudLK6ychKhzX_GGjk7JydhRaQBs&amp;scisig=AAGBfm0AAAAAYxjOfpUSteParaaZUb0Baq11kd8bT7oX&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Casper, S., Hadfield-Menell, D., Kreiman, G (2022). <a href="https://arxiv.org/abs/2209.02167">White-Box Adversarial Policies in Deep Reinforcement Learning</a>. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Huc79PTJfakJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClEwd-8QELrotZK-v9c:AFWwaeYAAAAAZQ24p9du_7iLiIbzO7OVFWqMLJg&amp;scisig=AFWwaeYAAAAAZQ24p0MWHmdvsz6DeVY2L-1xMxk&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Christoffersen, P.J.K., Haupt, A.A, Hadfield-Menell, D. (2022). <a href="https://arxiv.org/abs/2208.10469">Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL</a>. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:rctroivbpiAJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgUBYGTzEPyMg5PIHNw:AAGBfm0AAAAAYxjOBNz2-vpDhjCI_wJ1FUgMgTwUEa8f&amp;scisig=AAGBfm0AAAAAYxjOBPuUsEPypykSIeu3v7C_ZNMSKwx8&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Yew, R.J. and Hadfield-Menell, D. (2022). <a href="https://dl.acm.org/doi/10.1145/3514094.3534130">A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems</a>. In Proceedings of the 5th AAAI/ACM Conference on AI, Ethics, and Society. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Zy8cJGbw9QUJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgWTYX5AEPyMg45o47g:AAGBfm0AAAAAYwVu-7hfL7sgjbex8wF3U-g2nDKsY20o&amp;scisig=AAGBfm0AAAAAYwVu-y80HvtCEX2eXNg2NM7Ki7kE-BiC&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a>.</p>

<p>Casper, S., Nadeau, M., Hadfield-Menell, D, &amp; Kreiman, G (2022). <a href="https://arxiv.org/abs/2110.03605">Robust Feature-Level Adversaries are Interpretability Tools</a>. Advances in Neural Information Processing Systems, 35, 33093-33106. <a href="https://dblp.uni-trier.de/rec/journals/corr/abs-2110-03605.html?view=bibtex">BibTeX</a>.</p>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
<!--        -->
<!--        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>-->
      </footer>
    </div>
  </body>
</html>