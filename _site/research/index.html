<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=67ff6d9d51e2cef463b054469cfbeeb2c2e71844">
    <link rel="shortcut icon" type="image/x-icon" href="favicon_io/favicon.ico">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Algorithmic Alignment Group | Researching frameworks for human-aligned AI @ MIT CSAIL.</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Algorithmic Alignment Group" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Researching frameworks for human-aligned AI @ MIT CSAIL." />
<meta property="og:description" content="Researching frameworks for human-aligned AI @ MIT CSAIL." />
<link rel="canonical" href="https://thestephencasper.github.io/research/" />
<meta property="og:url" content="https://thestephencasper.github.io/research/" />
<meta property="og:site_name" content="Algorithmic Alignment Group" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Algorithmic Alignment Group" />
<script type="application/ld+json">
{"headline":"Algorithmic Alignment Group","@type":"WebPage","url":"https://thestephencasper.github.io/research/","description":"Researching frameworks for human-aligned AI @ MIT CSAIL.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/Algorithmic-Alignment-Lab/Algorithmic-Alignment-Lab.github.io">View on GitHub</a>
          

          <a href="/"><img style="float: left; padding-right: 20px; border: 0px;" src="/docs/assets/logo.png" width="68" height="68"></a><h1 id="project_title">Algorithmic Alignment Group</h1>
          <h3 id="project_tagline">Researching frameworks for human-aligned AI @ MIT CSAIL.</h3>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
<!--<h1>Group Name and Logo</h1>-->

<!--<head>-->
<!--    <meta charset="utf-8">-->
<!--    <link rel="stylesheet" href="_includes/styles.css">-->
<!--</head>-->

<center>
    <nav>
        <ul class="nav__links">
            <li><a href="/"><h3>Home</h3></a></li>
            <li><a href="/team/"><h3>Team</h3></a></li>
            <li><a href="/research/"><h3>Research</h3></a></li>
            <li><a href="/contact/"><h3>Contact</h3></a></li>
        </ul>
    </nav>
</center>

<h2 id="research">Research</h2>

<p>Find us on <a href="https://github.com/Algorithmic-Alignment-Lab">Github</a>.</p>

<h3 id="2022">2022</h3>

<p>Christoffersen, P.J.K., Haupt, A.A, Hadfield-Menell, D. (2022). <a href="https://arxiv.org/abs/2208.10469">Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL</a>.</p>

<p>Yew, R.J. and Hadfield-Menell, D. (2022). <a href="https://dl.acm.org/doi/10.1145/3514094.3534130">A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems</a>. In Proceedings of the 5th AAAI/ACM Conference on AI, Ethics, and Society. <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Zy8cJGbw9QUJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgWTYX5AEPyMg45o47g:AAGBfm0AAAAAYwVu-7hfL7sgjbex8wF3U-g2nDKsY20o&amp;scisig=AAGBfm0AAAAAYwVu-y80HvtCEX2eXNg2NM7Ki7kE-BiC&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en">BibTeX</a></p>

<p>Casper, S., Nadeau, M., Hadfield-Menell, D, &amp; Kreiman, G (2022). <a href="https://arxiv.org/abs/2110.03605">Robust Feature-Level Adversaries are Interpretability Tools</a>. <a href="https://dblp.uni-trier.de/rec/journals/corr/abs-2110-03605.html?view=bibtex">BibTeX</a></p>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
<!--        -->
<!--        <p class="copyright">Algorithmic Alignment Group maintained by <a href="https://github.com/Algorithmic-Alignment-Lab">Algorithmic-Alignment-Lab</a></p>-->
<!--        -->
<!--        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>-->
      </footer>
    </div>
  </body>
</html>