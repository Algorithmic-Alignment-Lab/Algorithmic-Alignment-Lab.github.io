<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://thestephencasper.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://thestephencasper.github.io/" rel="alternate" type="text/html" /><updated>2024-07-26T11:08:06+02:00</updated><id>https://thestephencasper.github.io/feed.xml</id><title type="html">Algorithmic Alignment Group</title><subtitle>Researching frameworks for human-aligned AI @ MIT CSAIL.</subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="https://thestephencasper.github.io/jekyll/update/2022/02/20/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2022-02-20T05:49:21+01:00</published><updated>2022-02-20T05:49:21+01:00</updated><id>https://thestephencasper.github.io/jekyll/update/2022/02/20/welcome-to-jekyll</id><content type="html" xml:base="https://thestephencasper.github.io/jekyll/update/2022/02/20/welcome-to-jekyll.html"><![CDATA[<!--<h1>Group Name and Logo</h1>-->

<!--<head>-->
<!--    <meta charset="utf-8">-->
<!--    <link rel="stylesheet" href="_includes/styles.css">-->
<!--</head>-->

<center>
    <nav>
        <ul class="nav__links">
            <li><a href="/"><h3>Home</h3></a></li>
            <li><a href="/team/"><h3>Team</h3></a></li>
            <li><a href="/research/"><h3>Research</h3></a></li>
            <li><a href="/contact/"><h3>Contact</h3></a></li>
        </ul>
    </nav>
</center>

<h1 id="example">Example</h1>

<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Home Team Research Contact]]></summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="https://thestephencasper.github.io/jekyll/update/2022/02/20/mechanistic_interpretability_challenges.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2022-02-20T05:49:21+01:00</published><updated>2022-02-20T05:49:21+01:00</updated><id>https://thestephencasper.github.io/jekyll/update/2022/02/20/mechanistic_interpretability_challenges</id><content type="html" xml:base="https://thestephencasper.github.io/jekyll/update/2022/02/20/mechanistic_interpretability_challenges.html"><![CDATA[<!--<h1>Group Name and Logo</h1>-->

<!--<head>-->
<!--    <meta charset="utf-8">-->
<!--    <link rel="stylesheet" href="_includes/styles.css">-->
<!--</head>-->

<center>
    <nav>
        <ul class="nav__links">
            <li><a href="/"><h3>Home</h3></a></li>
            <li><a href="/team/"><h3>Team</h3></a></li>
            <li><a href="/research/"><h3>Research</h3></a></li>
            <li><a href="/contact/"><h3>Contact</h3></a></li>
        </ul>
    </nav>
</center>

<h1 id="takeaways-from-the-mechanistic-interpretability-challenges">Takeaways from the Mechanistic Interpretability Challenges</h1>

<h2 id="plus-more-challenges-on-the-way">…plus more challenges on the way </h2>

<p>Stephen Casper, <a href="mailto:scasper@mit.edu">scasper@mit.edu</a></p>

<p>Spoilers ahead</p>

<p><a href="https://www.alignmentforum.org/posts/EjsA2M8p8ERyFHLLY/takeaways-from-the-mechanistic-interpretability-challenges">Crossposted from the AI Alignment Forum</a></p>

<h2 id="what-happened">What happened?</h2>

<p>The Mechanistic Interpretability Challenges (<a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/KSHqLzQscwJnv44T8">post</a> and <a href="https://github.com/thestephencasper/mechanistic_interpretability_challenge">GitHub</a>) were two challenges I posed in February as part of the <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7">Engineer’s Interpretability Sequence</a>. The first challenge was to find the pseudocode for the labeling function used to train a small CNN MNIST classifier. It was <a href="https://www.alignmentforum.org/posts/sTe78dNJDGywu9Dz6/solving-the-mechanistic-interpretability-challenges-eis-vii">solved</a> early last month. The second was to find the pseudocode for the labeling function used to train a one-layer transformer that classified pairs of numbers into two categories. It was <a href="https://www.alignmentforum.org/posts/k43v47eQjaj6fY7LE/solving-the-mechanistic-interpretability-challenges-eis-vii-1">solved</a> (with some reservations) late last month. Instead of finding the labeling function, the researchers who solved it obtained a mechanistic explanation of how the model worked and argued that the labeling function’s pseudocode would not be tractable to find from the model. </p>

<h2 id="thanks-to-stefan-marius-and-neel">Thanks to Stefan, Marius, and Neel</h2>

<p>Stefan Heimersheim and Marius Hobbhahn solved both challenges as a team. I and others have been impressed with their work. Meanwhile, Neel Nanda offered to contribute $500 to the prize pool for solving each challenge. Per the request of Stefan and Marius, a total of $1,500 has been donated by Neel and me to <a href="https://www.aisafetysupport.org/">AI Safety Support</a>.  </p>

<h2 id="why-these-challenges">Why These Challenges?</h2>

<p>In the original post on the challenges, I argued that solving them would be one of the first clear examples of mechanistic interpretability being used to solve a problem that was not specifically selected to be solvable with mechanistic interpretability. </p>

<p>Because it doesn’t treat models as black boxes, mechanistic interpretability is one of the potential solutions we might have for diagnosing and debugging insidious alignment failures. For example, if a model has a trojan or plans to make a treacherous turn once it detects that it’s in deployment, then these failures will be virtually undetectable from black-box access alone during training and development. </p>

<p>Mechanistic interpretability has been a reasonably high-profile research area for the past 6 years or so in the AI safety community. And it is currently undergoing a renewed surge of interest. However, I have tried to be <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3">critical</a> of the fact that much of the progress in mechanistic interpretability research has been on “<a href="https://en.wikipedia.org/wiki/Streetlight_effect">streetlight</a> interpretability” projects often with cherrypicked models and tasks. As a result of this, there is a risk that, if mechanistic interpretability continues to be a field full of cherrypicked and toy work, it may fail to produce methods that keep up with state-of-the-art applications of AI. Certainly, progress in mechanistic interpretability has not kept up with progress in AI as a whole, and despite all of the interest from the AI safety community, it lacks any big wins or many real-world applications at all that produce competitive tools for engineers solving real-world problems. </p>

<p>Hence the purpose of the mechanistic interpretability challenges: to provide challenges that aren’t able to be cherrypicked by those undertaking them. The hope has been that these challenges and others like it could offer a useful way of testing how useful approaches to interpretability are. The goal is to measure how promising specific methods and mechanistic interpretability itself are for truly reverse engineering models performing tasks that don’t happen to be under any particular streetlight. </p>

<h2 id="the-first-challenge-a-clear-win-for-mi">The First Challenge: A Clear Win for MI</h2>

<p>As is now public information, the MNIST CNN was trained on a labeling function that labeled images with small and large L1 distances to this image as a 1 while images with a medium L1 distance from it were labeled as a 0.</p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EjsA2M8p8ERyFHLLY/e5mcqatr96k5f74xnj4f" alt="" /></p>

<p>The <a href="https://www.alignmentforum.org/posts/sTe78dNJDGywu9Dz6/solving-the-mechanistic-interpretability-challenges-eis-vii">solution</a> was thorough. The network developed a set of “detectors” and “anti-detectors” for this image in the penultimate layer. It labeled anything that was detected or anti-detected as a 1 while labeling everything else as a 0. This seems to be an example of an instance in which a neural network developed a coherent, easily-explainable solution to a problem (albeit a toy one) that lent itself to good mechanistic interpretations – even without being specifically selected for this! Notably, the most compelling evidence from the solution involved a very clean application of causal scrubbing. I think this is a win for causal scrubbing as well. </p>

<h2 id="the-second-challenge-reasons-for-both-optimism-and-pessimism">The Second Challenge: Reasons for Both Optimism and Pessimism</h2>

<p>In the second challenge, the labeling function and learned solution looked like this. </p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EjsA2M8p8ERyFHLLY/tj7bhyaoesoaefdimbrc" alt="" /></p>

<p>As is now public information, the labeling function was:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p = 113
def label_fn(x, y):
    z = 0
    if x ** 2 &lt; np.sqrt(y) * 200:
        z += 3
    if y ** 2 &lt; np.sqrt(x) * 600:
        z -= 2
    z += int(x ** 1.5) % 8 - 5
    if y &lt; (p - x + 20) and z &lt; -3:
        z += 5.5
    if z &lt; 0:
        return 0
    else:
        return 1
</code></pre></div></div>

<p>The <a href="https://www.alignmentforum.org/posts/k43v47eQjaj6fY7LE/solving-the-mechanistic-interpretability-challenges-eis-vii-1">solution</a> to it was equally well done as the first. It made a strong case that the model computed its label by doing almost all of the work with the embeddings alone. The MLP layers implemented a simple function, and the attention layers did very little. </p>

<h3 id="what-this-solution-did-not-do">What This Solution Did Not Do</h3>

<p>First, I would have liked to see an explanation of why the transformer only seems to make mistakes near the parts of the domain where there are curved boundaries between regimes. Meanwhile, the network did a great job of learning the (transformed) periodic part of the solution that led to irregularly-spaced horizontal bars. Understanding why this is the case seems interesting but remains unsolved.</p>

<p><strong>Second, this solution did not solve the mechanistic interpretability challenge as it was posed.</strong> It did not find pseudocode for the labeling function, but instead made a strong case that it would not be tractable to find this. In this case, the network seemed to learn to label points by interpolating from nearby ones rather than developing an interesting, coherent internal algorithm. As a result, this seems to be a counterexample to some of the reasons that people are optimistic about mechanistic interpretability. </p>

<p>I think that streetlighting and cherrypicking in mechanistic interpretability may lead to a harmful notion that deep down, under the hood, neural networks are doing program induction. <strong>To the extent that neural networks do interpolation instead of program induction, then we should not be looking for the type of thing that the</strong> <a href="https://arxiv.org/abs/2301.05217"><strong>progress measures</strong></a> <strong>paper showed.</strong> This also seems to dampen optimism about <a href="https://www.alignmentforum.org/posts/YQALrtMkeqemAF5GX/another-list-of-theories-of-impact-for-interpretability#:~:text=doing%20something%20dangerous-,Microscope%20AI,-Instead%20of%20building">microscope AI</a> – even if one has an excellent mechanistic understanding of a model, it may not transfer to useful domain knowledge. In the wild, there is a lot of empirical (e.g. difficulties of mechanistic interpretability) and theoretical (e.g. <a href="https://arxiv.org/abs/1806.07572">NTK</a> theory) support for the idea that neural networks do not do program induction. Accordingly, I would <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/7TFJAvjYfMKxKQ4XS">argue</a> that <strong>we should expect very limited amounts of progress and little scalability from attempts to develop thorough prosaic, understandings of non-toy models performing non-cherrypicked tasks.</strong></p>

<p>Nonetheless, I was excited to declare this challenge solved, just with an asterisk.</p>

<h2 id="were-the-challenges-too-easy">Were the Challenges too Easy?</h2>

<p>I don’t think so. Stefan and Marius may have made them look easy, but they still spent quite a bit of effort on their solutions. Several others who attempted one or both of the challenges and contacted me found themselves stuck. However, I still did not expect that the challenges would be solved as well and as soon as they were. I thought they would stand for longer. </p>

<p>But, of course, these two challenges were completely toy. Future challenges and benchmarks should not be. </p>

<p>Two minor notes: (1) I regret providing the hint I did with the CNN challenge which showed the image that was used for the labeling function, but I am confident that Stefan and Marius would have solved it without the hint anyway. (2) As the person behind the challenges, I could have refused to consider the transformer challenge solved without pseudocode for the labeling function – demonstrating that this is hard to do was one of the main points of the challenge in the first place. But I do not think this would have been the right thing to do, and I fully believe that the solution was illuminating and thorough enough to deserve a win. I do not think there is very much reasonable room for improvement.</p>

<h2 id="these-are-just-one-of-the-ways-to-evaluate-interpretability-work">These are Just One of the Ways to Evaluate Interpretability Work</h2>

<p>In the past, I have expressed a lot of optimism for objective ways to evaluate approaches to interpreting models – something that is currently lacking in the space. There are <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/gwG9uqw255gafjYN4#If_we_want_interpretability_tools_to_help_us_do_meaningful__engineering_relevant_things_with_networks__we_should_establish_benchmarks_grounded_in_useful_tasks_to_evaluate_them_for_these_capabilities__">three ways</a> that interpretability tools can be evaluated. I wrote about them in <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/L5Rua9aTndviy8dvc#Benchmarking">EIS XI</a>:</p>

<ol>
  <li>
    <p><strong>Making novel predictions about how the system will handle interesting inputs.</strong> This is what we worked on in <a href="https://arxiv.org/abs/2302.10894">Casper et al. (2023)</a>. Approaches to benchmarking in this category will involve designing adversaries and detecting trojans. </p>
  </li>
  <li>
    <p><strong>Controlling what a system does by guiding manipulations to its parameters.</strong> Benchmarks based on this principle should involve implanting and/or removing trojans or changing other properties of interest. <a href="https://backdoorbench.github.io/">Wu et al. (2022)</a> provide benchmarks involving tasks with trojans that are somewhat related to this.</p>
  </li>
  <li>
    <p><strong>Abandoning a system that does a nontrivial task and replacing it with a simpler reverse-engineered alternative.</strong> Benchmarks for this might involve using interpretability tools to reconstruct the function that was used to design or supervise a network. This is the kind of thing that this challenge and <a href="https://arxiv.org/abs/2301.05062">Lindner et al. (2023)</a> focus on.</p>
  </li>
</ol>

<p>Moving forward, various benchmarks and competitions based on these three basic approaches might be very stimulating and healthy for the mechanistic interpretability community. Some of this seems to be catching on. For example, there has been a lot of interest in using <a href="https://arxiv.org/abs/2301.05062">Tracr</a> to build networks that have a known algorithmic function. I hope that we see continued interest in this and more. </p>

<p>Benchmarks offer feedback, concretize goals, and can spur coordinated research efforts. Benchmarks are not the real world, and it is important not to overfit to them, but they <a href="https://www.alignmentforum.org/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2">seem</a> to have an unparalleled ability to lead to engineering progress in a field. Benchmarks like CIFAR, ImageNet, GLUE, SuperGLUE, Big Bench, and others have led to so much progress in vision and language. So I hope that not too long from now, there are some widely-applicable benchmarks (or at least <em>types</em> of benchmarking tasks) used for interpretability, diagnostic, and debugging tools.</p>

<h2 id="more-non-toy-challenges-coming-soon">More (Non-Toy) Challenges Coming Soon</h2>

<p>Soon, my labmates, coauthors, and I will roll out a new competition for interpretability tools. It will be based on <a href="https://arxiv.org/abs/2302.10894">this paper</a> in which we evaluate interpretability tools based on their ability to help humans identify trojans in ImageNet models. There will be two challenges.</p>

<p>One will be to beat all of the methods we benchmarked in the paper. The challenge will be to produce a set of visualizations that are meant to help humans identify trojan triggers for the 12 trojans that we work with in the paper. Given a submission consisting of images and code to reproduce them, we will send them to knowledge workers just like we did with other methods in the paper. If your images beat the best result from the paper (which was a success rate of 0.49 on our 8-way multiple choice questions), you’ll beat the challenge.</p>

<p>The second challenge will be to, by any means necessary (mechanistic interpretability or not), identify the triggers for four secret trojans using only the ImageNet network with the trojans and the target class. </p>

<p><strong>For updates on this challenge, please</strong> <a href="https://docs.google.com/forms/d/e/1FAIpQLSfQiHha4eSJZI6ShawtA-UC4eptGqVZg_DdwfGStEBvu_8vrg/viewform?usp=sf_link"><strong>sign up here</strong></a> <strong>to receive a one-time email announcement later this year.</strong></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Home Team Research Contact]]></summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="https://thestephencasper.github.io/jekyll/update/2022/02/20/seven_strategies.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2022-02-20T05:49:21+01:00</published><updated>2022-02-20T05:49:21+01:00</updated><id>https://thestephencasper.github.io/jekyll/update/2022/02/20/seven_strategies</id><content type="html" xml:base="https://thestephencasper.github.io/jekyll/update/2022/02/20/seven_strategies.html"><![CDATA[<!--<h1>Group Name and Logo</h1>-->

<!--<head>-->
<!--    <meta charset="utf-8">-->
<!--    <link rel="stylesheet" href="_includes/styles.css">-->
<!--</head>-->

<center>
    <nav>
        <ul class="nav__links">
            <li><a href="/"><h3>Home</h3></a></li>
            <li><a href="/team/"><h3>Team</h3></a></li>
            <li><a href="/research/"><h3>Research</h3></a></li>
            <li><a href="/contact/"><h3>Contact</h3></a></li>
        </ul>
    </nav>
</center>

<h1 id="seven-strategies-for-tackling-the-hard-part-of-the-alignment-problem">Seven Strategies for Tackling the Hard Part of the Alignment Problem</h1>

<p>Stephen Casper, <a href="mailto:scasper@mit.edu">scasper@mit.edu</a></p>

<p><a href="https://www.alignmentforum.org/posts/amBsmfFK4NFDtkHiT/seven-strategies-for-tackling-the-hard-part-of-the-alignment">Crossposted from the AI Alignment Forum</a></p>

<p>Thanks to Michael Ripa for feedback.</p>

<h2 id="tldr">TL;DR</h2>

<ul>
  <li>There are two types of problems that AI systems can have: problems that we can observe during development and problems that we can’t. The hardest part of the alignment problem involves problems we can’t observe. </li>
  <li>I outline seven types of solutions for unobservable problems and taxonomize them based on whether they require that we know what failure looks like and how it happens mechanistically. 
    <ul>
      <li>Solutions that require both knowing what failure looks like and how it happens
        <ul>
          <li>Ambitious mechanistic interpretability</li>
          <li>Formal verification</li>
        </ul>
      </li>
      <li>Solutions that require knowing what failure looks like
        <ul>
          <li>Latent adversarial training</li>
          <li>Evals</li>
        </ul>
      </li>
      <li>Solutions that require knowing how failure happens
        <ul>
          <li>Mechanistic interpretability + heuristic model edits</li>
        </ul>
      </li>
      <li>Solutions that require neither
        <ul>
          <li>Anomaly detection</li>
          <li>Scoping models down</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>I argue that latent adversarial training, mechanistic interpretability + heuristic model edits, and scoping models down all may be highly important, tractable, and neglected and loft out a few ideas for new work. </li>
</ul>

<h2 id="two-ways-that-ai-systems-can-fail">Two ways that AI systems can fail</h2>

<p>Consider a simple way of dividing AI failures into two groups. </p>

<ol>
  <li><strong>Observable failures:</strong> Failures that we can encounter in training and development. These include failures on the train set, test set, some types of adversaries, or anything else that we might think to test our AI system on. These can be very bad, but they are problems that we can get feedback on – we can spot them and then work to fix them. </li>
  <li><strong>Unobservable failures:</strong> Failures that we will <em>not</em> encounter in training and development. These failures are somewhat scarier and harder to address because we will not have feedback to help us solve them. These can include trojans, some adversarial examples, misgeneralization, and deceptive alignment. </li>
</ol>

<p>Because one cannot get feedback on them, unobservable failures are the harder part of AI alignment, and this is why the AI safety community is especially interested in them. It is important not to ignore the importance and difficulty of fixing observable problems, and there might exist a very valid critique of the AI safety community’s (over)emphasis on unobservable ones. However, this post will focus on unobservable failures. </p>

<p>What are the ways that we can try to tackle unobservable failures? One good way may be to use models and data that are designed to avoid some of these problems in the first place (e.g. use better training data), but this will be outside the scope of this post. Instead, I will focus on ways to remove problems <em>given</em> a model and training set.  </p>

<h2 id="a-taxonomy-of-solutions">A taxonomy of solutions</h2>

<p>Consider two things that might make the process of addressing an unobservable problem in a model very difficult. </p>

<ol>
  <li><strong>Knowing what failure looks like.</strong> Can we recognize the bad behaviors when we see them? Lots of the time in ML, bad behavior is thought of as something that can be detected from a single output/action. But in reality, badness may be a property of a trajectory and may not be recognizable in simple ways. </li>
  <li><strong>Knowing how the failure happens.</strong> Can we figure out the internal mechanisms behind failures? This a difficult challenge because neural networks are hard to understand in terms of faithful mechanistic interpretations. </li>
</ol>

<p>Now consider which approaches to tackling unobservable failures depend on solving each of these two challenges.</p>

<h3 id="hard-approaches-that-require-both-knowing-what-failure-looks-like-and-how-it-happens">(Hard) Approaches that require both knowing what failure looks like and how it happens:</h3>

<ul>
  <li><strong>Ambitious mechanistic interpretability:</strong> This refers to anything that involves using mechanistic interpretability to actually figure out how the model will do something bad. Once this is done, then the model that will do bad things can be deleted or modified. </li>
  <li><strong>Formal verification:</strong> This refers to anything involving a proven bound on the probability of failures conditional on a particular set of specifications for using the model. In general, this is hard – the deep learning field struggles to prove practically-useful bounds for model failures. </li>
</ul>

<h3 id="medium-approaches-that-require-knowing-what-failure-looks-like">(Medium) Approaches that require knowing what failure looks like.</h3>

<ul>
  <li><strong>Latent adversarial training:</strong> <a href="https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training">Latent adversarial training</a> is an example of <a href="https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment">relaxed adversarial training</a> in which the model is trained under adversarial perturbations to its internal activations (instead of its inputs). Even if the inputs that trigger model failures are rare, there will likely be a substantial amount of neural circuitry dedicated to many unobservable failures (e.g. trojans, deceptive alignment). As long as bad behavior can be recognized when it happens, this relaxation of the adversarial training process could help to train that bad circuitry out of models without knowing what triggers it or how the network processes the bad response. </li>
  <li><strong>Evals:</strong> Black-box evals aren’t solutions to unobservable failures in theory. But evals can be informed by <em>white-box</em> analysis, and in reality, we also may be able to get a long way with good methodology and honeypotting.</li>
</ul>

<h3 id="medium-approaches-that-require-knowing-how-failure-happens">(Medium) Approaches that require knowing how failure happens.</h3>

<ul>
  <li><strong>Mechanistic interpretability + heuristic model edits:</strong> This refers to identifying model mechanisms or knowledge that seem likely to be involved in failures and then editing the model to address them. </li>
</ul>

<h3 id="easy-approaches-that-dont-require-either">(Easy) Approaches that don’t require either.</h3>

<ul>
  <li><strong>Mechanistic anomaly detection:</strong> this could allow us to find and flag examples that could be risky on the basis that the model internally handles them differently than typical inputs. </li>
  <li><strong>Scoping models down:</strong> If we are worried that we might accidentally be giving our models bad capabilities or intentions that will manifest off the train distribution, we may be able to get rid of many of these liabilities with tools to impair the model’s ability to do anything very coherent off distribution. For example, the model could be compressed or distilled on its own outputs so that it retains in-distribution performance and (not-so-catastrophically) forgets possibly-dangerous behaviors off distribution. </li>
</ul>

<h2 id="what-does-this-framework-suggest">What does this framework suggest?</h2>

<p>First, I have simply found this taxonomy useful for how I think about the hard part of alignment and the set of strategies we have for addressing it.</p>

<p>Next, I would not be surprised if we lived in a world in which the hard strategies – ambitious mechanistic interpretability and formal verification – just never pan out. I have <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7">written</a> in the past about how AI interpretability might be unproductive in some ways. This framing might help underscore some of these points. </p>

<p>In particular, I think this taxonomy helps to suggest that latent adversarial training, mechanistic interpretability + heuristic model edits, and scoping models down might be important, tractable, and neglected strategies. None seem to get a lot of current attention among AI safety researchers in deep learning, but all three seem to be tractable and have a high upside. (I’d also group evals and mechanistic anomaly detection in with these, but they seem slightly less neglected at the moment.)</p>

<h3 id="latent-adversarial-training">Latent adversarial training</h3>

<p>I think that it is possible for latent adversarial training to do most of what the mechanistic interpretability field is trying to achieve with less invested effort. The useful thing about latent adversarial training is that if you can solve the oversight problem, training under the <em>right</em> latent perturbations will, at least in theory, make the model robust to unobservable failures. Both of those things are much easier said than done. But ambitious mechanistic interpretability requires both of these plus a precise understanding mechanisms responsible for bad behavior plus using that understanding to address them somehow. It just seems strictly harder. </p>

<p>Right now, I am working with some others at MIT on latent adversarial training in deep learning. We are hoping to use it to improve robustness for high-stakes reliability and to help remove/resist trojan implantation. Some work already exists that involves adversarial perturbations to model latents (e.g. <a href="https://arxiv.org/abs/2104.01575">this paper</a>), but to our knowledge, we are working to be the first to use it for controlling OOD liabilities in deep learning. I think that much more work could be done in the future as well.</p>

<h3 id="mechanistic-interpretability--heuristic-model-edits">Mechanistic Interpretability + Heuristic Model Edits</h3>

<p>Getting rid of knowledge in models that might be risky could be very useful for safety. It might be nice to be able to take a language model and erase its knowledge entirely on some risky domain. For instance, maybe future versions of ChatGPT should have all of their knowledge of biotech erased so that they cannot effectively help to make a bioweapon. How might we go about this?</p>

<p>Recently, there has been <a href="https://arxiv.org/abs/2202.05262">a</a> <a href="https://arxiv.org/abs/2210.07229">lot</a> <a href="https://arxiv.org/pdf/2301.04213.pdf">of</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=dnZ8udEAAAAJ&amp;sortby=pubdate&amp;citation_for_view=dnZ8udEAAAAJ:sSrBHYA8nusC">interest</a> in editing factual associations in models. Prior works have modeled factual knowledge as labeled arrows between concepts, e.g. eiffel tower –&gt; located in –&gt; Paris. Maybe using some strategy based on these tools would work well, and we should find out. But not all types of knowledge seem most easily described as sets of simple, directed relations between entities (e.g. knowledge within a broad domain like biotech). So it may be useful to work on methods to completely erase a model’s knowledge in large domains. Methods like this might be great to add to the alignment toolbox. </p>

<h3 id="scoping-models-down">Scoping Models Down</h3>

<p>This may be an underrated approach to alignment – just making the AI more narrow. Right now, state-of-the-art AI systems are developed under a pretrain-then-finetune paradigm. But it might be valuable from a safety perspective to move to a pretrain-then-finetune-then-scope-down one. </p>

<p>One useful approach for scoping models down could involve using network compression. If a model does something bad off distribution (like executing a precise plot to betray you), the forward passes through it are going to be mechanistically different in some way compared to the passes that are observed during development. Using a mechanistic compression technique to get rid of neural circuitry which is not needed for in-distribution performance might disrupt the ability of the network to make the same computations off distribution. This could incidentally remove much of the circuitry involved in some types of OOD failures without ever needing to identify those failures in the first place. </p>

<p>Network pruning has been <a href="https://arxiv.org/abs/2103.03014">observed</a> to harm OOD performance before, and from a safety standpoint, this may be a feature – not a bug. Right now could be a good time to work on scoping models down for alignment via compression because of recent interest in causal compression methods (like <a href="https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing">causal scrubbing</a> or <a href="https://arxiv.org/abs/2304.14997">ACDC</a>). These methods are being used for mechanistic anomaly detection and automating “interpretations” of networks, but we might be able to get a long way with just the compressed networks themselves.</p>

<p>Another approach for scoping models down could be inspired by the <a href="https://arxiv.org/abs/1909.08383">continual learning</a> literature which aims to make models better at learning new tasks without forgetting old ones. If we want to scope a model down, the whole point will be to make it forget previously learned information that isn’t related to the in-distribution data we supervisedly finetune it on. In this case, <a href="https://arxiv.org/abs/2306.00427">continued training</a>, <a href="https://arxiv.org/abs/2211.12044?fbclid=IwAR1Qupu_GUsQDmdK2o7YAMQWrkUv38n5Kxff9w_9FyIQpK1iU6dIGLb9UhE">several</a> <a href="https://arxiv.org/abs/2306.00427">approaches</a> <a href="https://arxiv.org/abs/2303.10594?fbclid=IwAR0_XiZiSSM5BElLcCahEczDbody9ndLZLW2ELDOvytqQSmO0F6ifN39zPQ">using</a> <a href="https://arxiv.org/abs/2101.05930?fbclid=IwAR1IsetxvmzV6qze9411LK08CDt9mxC47_K9_eAezjVXnDq5N9xTJ2lmFkY">distillation</a>, <a href="https://arxiv.org/abs/1805.09092">excitation dropout</a>, and <a href="https://arxiv.org/abs/2212.04687?fbclid=IwAR0dol7NBf8_Kej9eLc19lkMr4CoIlYXfp3h2X11GE6pafgyW5SPKENGC68">unlearning followed by re-learning</a> seem promising. However, there seems to be more work needed to develop methods and benchmark them. I think that one promising idea could also be to test how useful taking <a href="https://arxiv.org/abs/1909.08383">existing continual learning techniques</a> and doing the opposite could encourage the type of plasticity/forgetting that would help to scope models down. Trojan removal seems like a promising testbed for this type of work. </p>

<h2 id="paying-the-alignment-tax">Paying the Alignment Tax</h2>

<p>When I have talked to people about some of these approaches, they sometimes bring up the concern that this might make the models bad and less competitive. But that’s kind of the point. AI safety is much more about a lack of certain capabilities than having certain capabilities. Any aligned model is going to be worse in some way than the easier, misaligned alternative. That’s just the alignment tax. For example, adversarial training and content filters make models worse too, and we tolerate that ChatGPT is often stubbornly unhelpful. It’s just the price to pay.</p>

<p>Let me know if you are interested in working on any of these ideas :)</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Home Team Research Contact]]></summary></entry></feed>